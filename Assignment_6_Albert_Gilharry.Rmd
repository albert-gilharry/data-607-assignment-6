---
title: 'DATA607 Assignment 6: Web APIs'
author: "Albert Gilharry"
date: "29 March 2018"
output:
  html_document:
    css: ./css.css
    highlight: pygments
    pdf_document: default
    theme: cerulean
    toc: yes
  pdf_document:
    toc: yes
always_allow_html: yes
---

```{r setup, include=FALSE}
library ("httr")
library("tidyverse")
library("splitstackshape")
library("jsonlite")
library("DT")

api_key = "57cc982821bc4d0fb06b840ef72a9be9"
```

## Intro

<div id = "comment">
**We were tasked to choose one of the New York Times APIs, construct an interface in R to read in the JSON data, and transform it to an R dataframe.**
</div>

<div id = "solution">
**I decided to use the Most Popular API. This API returns a list of New York Times Articles based on shares, emails, and views.**
</div>


## Load JSON Data into Data Frame

<div id = "comment">
I will use to NYT API to get the most frequently shared (by email) articles over the last 30 days.

The NYT API returns a maximum of 20 articles per request.

We must paginate these request using the `offset` parameter.

We must also ensure that we do not exceed the rate limit of 5 requests per second.
</div>

<div id = "solution">
```{r load-json, warning=FALSE}
# load JSON data into data frame
url = paste0("https://api.nytimes.com/svc/mostpopular/v2/mostemailed/all-sections/30.json?api-key=", api_key)
articles <- fromJSON(url, flatten = TRUE) %>% data.frame()
pages <- floor(articles$num_results[1]/20) 

for(i in 1:pages){
  print(paste0("Loading page ", i, " of ", pages, "." ))
  page <- fromJSON(paste0(url, "&offset=",i*20), flatten = TRUE) %>% data.frame()
  articles <- bind_rows(articles,page) # build data frame incrementally
  Sys.sleep(1) #Stay within usage rate limits
}

# add row id to utilize less memory when transforming
articles <- rowid_to_column(articles,'id')
```

</div>

## Preview Resulting Data Table

<div id = "solution">
```{r preview-articles}
print(names(articles))

datatable(head(select(articles, results.title, results.source, results.des_facet, results.published_date)), options = list(filter = FALSE))
```
</div>


## Tidy & Transform Data

<div id = "comment">
I would like to do an exploration to find out the tags/keywords with the highest proportion of shares by email.
The descriptive tags for each article is stored in a character vector with multiple elements in `results.des_facet`.
The keywords will need to be separated and then placed in a long format to facilitate downstream analysis. 
</div>

<div id = "solution">
```{r tidy-keywords}
keywords <- select(articles, `id`, `results.des_facet`)  %>% 
  group_by(`id`) %>% summarize(keyword = paste(unlist( `results.des_facet` ),collapse = ","))  %>%
  cSplit("keyword", sep = ",", direction = "long")
```
</div>


## Preview Transformed Data

```{r preview-keywords}
datatable(keywords, options = list(filter = FALSE))
```


## Visualization: Top 20 Email Shares by Keyword

<div id = "solution">
```{r plot-keywords}
  group_by(keywords, keyword) %>%
  summarize(count = n())  %>%
  arrange(desc(count)) %>%
  top_n(20,count) %>%
  ggplot(aes(x = reorder(keyword, count), y = count,  fill=keyword, label = count)) + 
  geom_histogram( stat='identity', show.legend = F ) + 
  geom_text(size = 2, position = position_stack(vjust = 0.5)) + 
  coord_flip() +  
  labs( title = "Top 20 Email Shares by Keyword", x = "Keywords/Tags", y = "Frequency" ) +
  theme(plot.title = element_text(hjust = 2))
```
</div>


## Conclusion
<div id = "comment">
The results show that the top NYT Articles shared by email were dominated by politics. 3 of the top 20 tags were related to Gun violence.
It would be great for more media houses to provide such an API service so that we can compare and contrast trending feeds across various sources.
</div>