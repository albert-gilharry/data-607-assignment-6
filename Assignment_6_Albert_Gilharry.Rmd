---
title: 'DATA607 Assignment 6: Web APIs
author: "Albert Gilharry"
date: "29 March 2018"
output:
  html_document:
    css: ./css.css
    highlight: pygments
    pdf_document: default
    theme: cerulean
    toc: yes
  pdf_document:
    toc: yes
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro

<div id = "comment">
**We were tasked to choose one of the New York Times APIs, construct an interface in R to read in the JSON data, and transform it to an R dataframe.**
</div>

<div id = "solution">
**I decided to use the Most Popular API. This API returns a list of New York Times Articles based on shares, emails, and views.**
</div>

## Load Libraries

```{r load-libraries}
library ("httr")
library("tidyverse")
library("splitstackshape")
library("jsonlite")
library("stringr")
library("DT")
```

## Load JSON Data into Data Frame

<div id = "comment">
The NYT API only returns a maximum of 20 articles per request.
We must paginate these request using the `offset` parameter.
We must also ensure that we do not exceed the rate limit of 5 requests per second.
</div>

<div id = "solution">
```{r}
# load JSON data into data frame
url = "https://api.nytimes.com/svc/mostpopular/v2/mostemailed/all-sections/30.json?api-key=57cc982821bc4d0fb06b840ef72a9be9"
articles <- fromJSON(url, flatten = TRUE) %>% data.frame()
pages <- floor(articles$num_results[1]/20) 

for(i in 1:pages){
  page <- fromJSON(paste0(url, "&offset=",i*20), flatten = TRUE) %>% data.frame()
  articles <- bind_rows(articles,page) # build data frame incrementally
  Sys.sleep(1) #Stay within usage rate limits
}

# add row id to utilize less memory when transforming
articles <- rowid_to_column(articles,'id')
```

</div>

## Preview Resulting Data Table

<div id = "solution">
```{r}
print(names(articles))


datatable(head(select(articles, results.title, results.source, results.des_facet, results.published_date)), options = list(filter = FALSE),filter="none")
```
</div>


## Tidy & Transform Data

<div id = "comment">
I would like to do an exploration to find out the tags/keywords with the highest proportion of shares by email.
The descriptive tags for each article is stored in a character vector with multiple elements in `results.des_facet`.
The keywords will need to be separated and then placed in a long format to facilitate downstream analysis. 
</div>

<div id = "solution">
```{r}
keywords <- select(articles, `id`, `results.des_facet`)  %>% 
  group_by(`id`) %>% summarize(keyword = paste(unlist( `results.des_facet` ),collapse = ","))  %>%
  cSplit("keyword", sep = ",", direction = "long")
```
</div>


## Preview Transformed Data

```{r}
datatable(head(keywords), options = list(filter = FALSE),filter="none")
```


## Visualize: Top 20 Email Shares by Keyword

<div id = "solution">
```{r}
  group_by(keywords, keyword) %>%
  summarize(count = n())  %>%
  arrange(desc(count)) %>%
  top_n(20,count) %>%
  ggplot(aes(x = reorder(keyword, count), y = count,  fill=keyword, label = count)) + 
  geom_histogram( stat='identity', show.legend = F ) + 
  geom_text(size = 2, position = position_stack(vjust = 0.5)) +
  coord_flip() +  
  labs( title = "Top 20 Email Shares by Keyword", x = "Keywords/Tags", y = "Frequency" )
```
</div>


## Conclusion
<div id = "comment">
The results show that the top NYT Articles shared by email were dominated by politics. 3 of the top 20 tags were related to Gun violence.
The NYT API was very nice to work with in R. This API may be applicable 
</div>